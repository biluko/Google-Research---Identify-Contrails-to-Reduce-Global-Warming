{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./mmsegmentation-030/080900_upernet_convnext_l_fold0_bs8_2.5e-4_train512_test512_noflip_adamwsw_ce10_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./mmsegmentation-030/080100_upernet_convnext_l_fold0_bs8_2.5e-4_train512_test512_noflip_adamwsw_ce10_config.py\n",
    "\n",
    "# work_dir = f'/home/br/workspace/GRIC/output/080414'\n",
    "\n",
    "num_classes = 2\n",
    "out_channels = 1\n",
    "threshold = 0.8\n",
    "class_weight = [10]\n",
    "fold = 0 \n",
    "\n",
    "norm_cfg = dict(type='BN', requires_grad=True) # 分割框架通常使用 SyncBN\n",
    "custom_imports = dict(imports='mmcls.models', allow_failed_imports=False)\n",
    "\n",
    "# checkpoint_file = 'https://download.openmmlab.com/mmclassification/v0/convnext/downstream/convnext-xlarge_3rdparty_in21k_20220301-08aa5ddc.pth' # xlarge\n",
    "\n",
    "checkpoint_file = 'https://download.openmmlab.com/mmclassification/v0/convnext/downstream/convnext-large_3rdparty_in21k_20220301-e6e0ea0a.pth' # large\n",
    "\n",
    "# checkpoint_file = 'https://download.openmmlab.com/mmclassification/v0/convnext/downstream/convnext-base_3rdparty_in21k_20220301-262fd037.pth' # base\n",
    "\n",
    "model = dict(\n",
    "    type = 'EncoderDecoder',\n",
    "    pretrained = None,\n",
    "\n",
    "    backbone = dict(\n",
    "        type='mmcls.ConvNeXt',\n",
    "        arch='large', #### base large xlarge\n",
    "        out_indices=[0, 1, 2, 3],\n",
    "        drop_path_rate=0.4,\n",
    "        layer_scale_init_value=1.0,\n",
    "        gap_before_final_norm=False,\n",
    "        init_cfg=dict(\n",
    "            type='Pretrained', checkpoint=checkpoint_file,\n",
    "            prefix='backbone.')\n",
    "            ),\n",
    "\n",
    "    decode_head = dict(\n",
    "        type='UPerHead', # 解码头(decode head)的类别。\n",
    "        # in_channels=[256, 512, 1024, 2048], # xlarge\n",
    "        in_channels=[192, 384, 768, 1536], # large\n",
    "        # in_channels=[128, 256, 512, 1024], # base\n",
    "        in_index=[0, 1, 2, 3], # 选择特征图的索引\n",
    "        pool_scales=(1, 2, 3, 6),\n",
    "        channels=512, # 解码头中间态(intermediate)的通道数。\n",
    "        dropout_ratio=0.1,  # 进入最后分类层(classification layer)之前的 dropout 比例。\n",
    "        num_classes=num_classes, # 分割前景的种类数目。 (仅二分类问题中为2，前景和背景)\n",
    "        out_channels=out_channels, # 辅助头输出的通道数。\n",
    "        threshold=threshold, # 二分类时预测的阈值，仅在推理时有效。\n",
    "        norm_cfg=norm_cfg,  # 归一化层的配置项。\n",
    "        align_corners=False, # 解码里调整大小(resize)的 align_corners 参数。\n",
    "        loss_decode=dict(type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0, class_weight=class_weight)\n",
    "        # loss_decode=dict(type='FocalLoss', use_sigmoid=True, gamma=2.0, alpha=[1.0], loss_weight=1.0, class_weight=class_weight),\n",
    "        # loss_decode=dict(type='DiceLoss', loss_weight=1.0, class_weight=[10, 0.1]), # , class_weight=[10]\n",
    "        ),\n",
    "\n",
    "    auxiliary_head = dict(\n",
    "        type='FCNHead', # 辅助头(auxiliary head)的种类。可用选项请参考 mmseg/models/decode_heads。\n",
    "        # in_channels=1024, # xlarge\n",
    "        in_channels=768, # large\n",
    "        # in_channels=512, # base\n",
    "        in_index=2, # 被选择的特征图(feature map)的索引。\n",
    "        channels=256, # 辅助头中间态(intermediate)的通道数。\n",
    "        num_convs=1, # FCNHead 里卷积(convs)的数目. 辅助头里通常为1。\n",
    "        concat_input=False, # 在分类层(classification layer)之前是否连接(concat)输入和卷积的输出。\n",
    "        dropout_ratio=0.1, # 进入最后分类层(classification layer)之前的 dropout 比例。\n",
    "        num_classes=num_classes, # 分割前景的种类数目。 (仅二分类问题中为2，前景和背景)\n",
    "        out_channels=out_channels, # 辅助头输出的通道数。\n",
    "        threshold=threshold, # 二分类时预测的阈值，仅在推理时有效。\n",
    "        norm_cfg=norm_cfg,  # 归一化层的配置项。\n",
    "        align_corners=False, # 解码里调整大小(resize)的 align_corners 参数。\n",
    "        loss_decode=dict(type='CrossEntropyLoss', use_sigmoid=True, loss_weight=0.4, class_weight=class_weight)\n",
    "        # loss_decode=dict(type='FocalLoss', use_sigmoid=True, gamma=2.0, alpha=[1.0], loss_weight=0.4, class_weight=class_weight),\n",
    "        # loss_decode=dict(type='DiceLoss', loss_weight=0.4, class_weight=[10, 0.1]), # , class_weight=[10]\n",
    "        ),\n",
    "\n",
    "    # model training and testing settings\n",
    "    train_cfg = dict(),\n",
    "    test_cfg = dict(mode='whole'), # 测试模式， 选项是 'whole' 和 'sliding'. 'whole': 整张图像全卷积(fully-convolutional)测试。 'sliding': 图像上做滑动裁剪窗口(sliding crop window)。\n",
    ")\n",
    "\n",
    "\n",
    "# dataset settings\n",
    "dataset_type = 'CustomDataset' # 数据集类型 CustomDataset NpyDataset\n",
    "data_root = '../input/mmseg_data/' # 数据的根路径\n",
    "\n",
    "classes = ['BG', 'contrails']\n",
    "palette = [[0,0,0], [255,0,0]]\n",
    "\n",
    "img_norm_cfg = dict(\n",
    "    # mean=[0,0,0], std=[1,1,1], to_rgb=True\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], to_rgb=True\n",
    ")\n",
    "\n",
    "albu_train_transforms = [\n",
    "    dict(type='RandomBrightnessContrast', p=0.5),\n",
    "]\n",
    "\n",
    "\n",
    "train_img_size = [(512,512)]\n",
    "test_img_size = [(512,512)]\n",
    "\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', reduce_zero_label=False),\n",
    "    # 多尺度训练\n",
    "    dict(type='Resize', \n",
    "    img_scale=train_img_size,\n",
    "    ),\n",
    "    ####\n",
    "    dict(type='RandomFlip', prob=0.00000001, direction='horizontal'),\n",
    "    # dict(type='RandomFlip', prob=0.5, direction='vertical'),\n",
    "    # dict(type='PhotoMetricDistortion'),\n",
    "    dict(type='Normalize', **img_norm_cfg),\n",
    "    # dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_semantic_seg']), # 决定数据里哪些键被传递到分割器里的流程。\n",
    "]\n",
    "\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=test_img_size,\n",
    "        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n",
    "        flip=False, # 测试时是否翻转图像。\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True), # 使用改变图像大小的数据增广\n",
    "            dict(type='RandomFlip'), # 考虑到 RandomFlip 已经被添加到流程里，当 flip=False 时它将不被使用。\n",
    "            dict(type='Normalize', **img_norm_cfg),  # 归一化配置项，值来自 img_norm_cfg。\n",
    "            # dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255),\n",
    "            dict(type='ImageToTensor', keys=['img']), # 将图像转为张量\n",
    "            dict(type='Collect', keys=['img']), # 收集测试时必须的键的收集流程。\n",
    "        ])\n",
    "]\n",
    "\n",
    "img_suffix = '.npy'\n",
    "fore_str = \"_fore\" # _fore\n",
    "data = dict(\n",
    "    samples_per_gpu = 8,\n",
    "    workers_per_gpu = 4,\n",
    "\n",
    "    train = dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        img_dir='images',\n",
    "        ann_dir='labels',\n",
    "        img_suffix=img_suffix,\n",
    "        seg_map_suffix='.png',\n",
    "        split=f\"splits/fold_{fold}{fore_str}.txt\",\n",
    "        classes=classes,\n",
    "        palette=palette,\n",
    "        pipeline=train_pipeline,\n",
    "        ),\n",
    "\n",
    "    val = dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        img_dir='images',\n",
    "        ann_dir='labels',\n",
    "        img_suffix=img_suffix,\n",
    "        seg_map_suffix='.png',\n",
    "        split=f\"splits/holdout_{fold}.txt\",\n",
    "        classes=classes,\n",
    "        palette=palette,\n",
    "        pipeline=test_pipeline),\n",
    "\n",
    "    test = dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        test_mode=True,\n",
    "        img_dir='images',\n",
    "        ann_dir='labels',\n",
    "        img_suffix=img_suffix,\n",
    "        seg_map_suffix='.png',\n",
    "\n",
    "        classes=classes,\n",
    "        palette=palette,\n",
    "        pipeline=test_pipeline))\n",
    "\n",
    "\n",
    "\n",
    "# yapf:disable\n",
    "log_config = dict(\n",
    "    interval = 100, # 打印日志的间隔\n",
    "    hooks = [ # 训练期间执行的钩子\n",
    "        dict(type='TextLoggerHook', by_epoch=False),\n",
    "        # dict(type='MMSegWandbHook', by_epoch=False, # 还支持 Wandb 记录器，它需要安装 `wandb`。\n",
    "        #      init_kwargs={'entity': \"OpenMMLab\", # 用于登录wandb的实体\n",
    "        #                   'project': \"mmseg\", # WandB中的项目名称\n",
    "        #                   'config': cfg_dict}), # 检查 https://docs.wandb.ai/ref/python/init 以获取更多初始化参数\n",
    "    ])\n",
    "# yapf:enable\n",
    "dist_params = dict(backend='nccl') # 用于设置分布式训练的参数，端口也同样可被设置。\n",
    "log_level = 'INFO' # 日志的级别。\n",
    "load_from = None  # 从一个给定路径里加载模型作为预训练模型，它并不会消耗训练时间。\n",
    "resume_from = None # 从给定路径里恢复检查点(checkpoints)，训练模式将从检查点保存的轮次开始恢复训练。\n",
    "workflow = [('train', 1)] # runner 的工作流程。 [('train', 1)] 意思是只有一个工作流程而且工作流程 'train' 仅执行一次。\n",
    "cudnn_benchmark = True  # 是否是使用 cudnn_benchmark 去加速，它对于固定输入大小的可以提高训练速度。\n",
    "\n",
    "# optimizer\n",
    "optimizer = dict(\n",
    "    constructor='LearningRateDecayOptimizerConstructor',\n",
    "    type='AdamW',\n",
    "    lr=2.5e-4, ####\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.05,\n",
    "    paramwise_cfg={\n",
    "        'decay_rate': 0.9,\n",
    "        'decay_type': 'stage_wise',\n",
    "        'num_layers': 12\n",
    "    }\n",
    "    )\n",
    "\n",
    "\n",
    "optimizer_config = dict(type='Fp16OptimizerHook', loss_scale='dynamic')\n",
    "# learning policy\n",
    "lr_config = dict(\n",
    "    policy='poly', # 调度流程的策略，同样支持 Step, CosineAnnealing, Cyclic 等.\n",
    "    warmup='linear',\n",
    "    warmup_iters=1500,\n",
    "    warmup_ratio=1e-6,\n",
    "    power=1.0, # 多项式衰减 (polynomial decay) 的幂。\n",
    "    min_lr=0.0, # 用来稳定训练的最小学习率。\n",
    "    by_epoch=False)\n",
    "\n",
    "# runtime settings\n",
    "runner = dict(type='EpochBasedRunner', max_epochs=30)\n",
    "checkpoint_config = dict(by_epoch=True, interval=1, save_last=True, max_keep_ckpts=15)\n",
    "evaluation = dict(interval=1, metric=['mDice'], pre_eval=True)\n",
    "\n",
    "fp16 = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./mmsegmentation-030/\n",
    "!python ./tools/train.py ./080812_upernet_convnext_l_fold1_bs8_2.5e-4_train512_test512_adamwsw_ce10_config.py --gpu-id 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
